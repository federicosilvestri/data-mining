{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM - Data understanding [TASK 1.1]\n",
    "\n",
    "Exploring the dataset with analytical tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN Only with COLAB\n",
    "\n",
    "This cell will setup notebook for running on Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://FedericoSilvestri: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@github.com/federicosilvestri/data-mining.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd data-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import sys\n",
    "import logging as lg\n",
    "\n",
    "root = lg.getLogger()\n",
    "root.setLevel(lg.INFO)\n",
    "\n",
    "handler = lg.StreamHandler(sys.stdout)\n",
    "handler.setLevel(lg.DEBUG)\n",
    "formatter = lg.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "pd.isna()\n",
    "pandarallel.initialize(\n",
    "    progress_bar=True,\n",
    "    use_memory_fs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Fetching the dataset using our native python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fetch_dataset\n",
    "\n",
    "dataset = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n",
    "\n",
    "Show `users.csv` information: types of data and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dataset['users.csv'].copy() # make a copy\n",
    "\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display lang values\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have:\n",
    "\n",
    "1. `xx-lc`\n",
    "2. `Select Language...`\n",
    "\n",
    "That are not a valid language.\n",
    "We have decided to use iso639-1 Python library to detect valid languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display BOT values\n",
    "# 0 -> it's a human!\n",
    "# 1 -> it's a bot!\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have clean data for `bot` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets\n",
    "\n",
    "Show `tweets.csv information: types of data and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dataset['tweets.csv'].copy()\n",
    "\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality assessment and data cleaning\n",
    "\n",
    "In these cells we are going to understand and clean the data of two datasets.\n",
    "The analysis performs:\n",
    "\n",
    "1. Replacement of null values with median if type is numerical, mode if type is categorical and **outlier** timestamp value if type is datetime.\n",
    "2. Deletion of rows that has a ratio between valid values and invalid values `< k` where `k` is a param with default value 60%.\n",
    "3. Understand and replace categorical value based on their domain. For example, the language column contains invalid language codes, and we replace them with the mode value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definition for outlier values\n",
    "min_date = pd.Timestamp('2006-03-21') # the date when Twitter has started the activity.\n",
    "max_date = pd.Timestamp('2022-09-28') # the date when dataset has been collected.\n",
    "\n",
    "# OUTLIER constants\n",
    "OUTLIER_TIMESTAMP = pd.Timestamp('1800-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_invalid_rows(df, column_validators, ratio=0.6):\n",
    "    #\n",
    "    # This function is a generic function, that performs cleaning of rows that are invalid.\n",
    "    # We define row as invalid if the ratio between valid and invalid attributes \n",
    "    # is greater than `ratio` parameter.\n",
    "    # \n",
    "    # The validation of single attribute is entrusted to the combination of \n",
    "    # lambda function named `validator` and the fact that the attribute is nan.\n",
    "    #\n",
    "    n_null_items = int(len(column_validators) * ratio)\n",
    "    \n",
    "    def check_invalid_rows_callback(row):\n",
    "        count = 0\n",
    "        for head, validator in column_validators:\n",
    "            value = row[head]\n",
    "            if pd.isnull(value) or (validator is not None and validator(value)):\n",
    "                count += 1\n",
    "        return count > n_null_items\n",
    "\n",
    "    outliers = df.parallel_apply(check_invalid_rows_callback, axis=1)\n",
    "    return df[~outliers], sum(outliers)\n",
    "\n",
    "# Definition of generic lambda validator functions\n",
    "check_int = lambda label: not bool(re.search(r'^(\\d)+(\\.0+)?$', str(label))) # checks, using regex if attribute is integer\n",
    "check_positive_int = lambda label: check_int(label) or float(label) < 0 # checks if label is positive\n",
    "check_date = lambda label: pd.Timestamp(label) < min_date or pd.Timestamp(label) > max_date # checks timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langcodes import tag_is_valid # import the library for ISO639-1 codes\n",
    "\n",
    "# For each column define a validator.\n",
    "column_validators = [\n",
    "    ('id', check_int),\n",
    "    ('name', None),\n",
    "    ('lang', tag_is_valid),\n",
    "    ('bot', lambda label: label == '1' or label == '0'),\n",
    "    ('statuses_count', check_int),\n",
    "    ('created_at', check_date),\n",
    "]\n",
    "\n",
    "#\n",
    "# Execute the cleaning function.\n",
    "#\n",
    "users, deleted_rows = clean_invalid_rows(users, column_validators)\n",
    "lg.info(f\"Deleted rows {deleted_rows} ({deleted_rows / len(users)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replacement of invalid names\n",
    "#\n",
    "invalid_names = users['name'].map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_names)} records, i.e. {sum(invalid_names) / len(invalid_names) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Explore bot column\n",
    "#\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all values of column bot are 0,1 so we can convert it into boolean field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replacement of invalid languages\n",
    "#\n",
    "\n",
    "# first normalize to lower case all langs\n",
    "users['lang'] = users['lang'].str.lower()\n",
    "\n",
    "# calculate the mode for this categorical value\n",
    "user_lang_mode = users['lang'].mode()[0]\n",
    "\n",
    "# lambda function for substition\n",
    "lang_subst_lambda = lambda x: x if tag_is_valid(x) else user_lang_mode\n",
    "\n",
    "# execute substitution\n",
    "users['lang'] = users['lang'].map(lang_subst_lambda)\n",
    "\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a constant that marks an attribute as an outlier.\n",
    "#\n",
    "\n",
    "def filter_datetime(df, att):\n",
    "    def parse_and_check_datetime(el):\n",
    "        try:\n",
    "            datetime = pd.Timestamp(el) # parse datetime as Timestamp\n",
    "            # checks validity\n",
    "            if datetime < min_date or datetime > max_date:\n",
    "                # is an outlier\n",
    "                return OUTLIER_TIMESTAMP\n",
    "            else:\n",
    "                # is not an outlier\n",
    "                return datetime\n",
    "        except ValueError:\n",
    "            # cannot parse as timestamp, it's an outlier\n",
    "            return OUTLIER_TIMESTAMP\n",
    "    df[att] = df[att].parallel_map(parse_and_check_datetime)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter to datetime column\n",
    "users = filter_datetime(users, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Handling the statuses_count column.\n",
    "#\n",
    "status_count_median = users['statuses_count'].median()\n",
    "\n",
    "# replace the null values with median\n",
    "users['statuses_count'].fillna(status_count_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "users = users.astype({\n",
    "    'id': 'int64',\n",
    "    'name': 'string',\n",
    "    'lang': 'string',\n",
    "    'bot': 'bool',\n",
    "    'statuses_count': 'int64',\n",
    "    'created_at': 'datetime64[ns]'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Removing duplicate records.\n",
    "#\n",
    "initial_ds_len = len(users)\n",
    "users = users.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(users)} duplicates record that are {(initial_ds_len - len(users)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the columns are now validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "users[['name', 'lang', 'bot', 'statuses_count', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a function for validating text of tweet.\n",
    "#\n",
    "check_text = lambda x: not x or len(str(x)) <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "column_validators = [\n",
    "    ('id', check_positive_int),\n",
    "    ('user_id', check_positive_int),\n",
    "    ('retweet_count', check_positive_int),\n",
    "    ('reply_count', check_positive_int),\n",
    "    ('favorite_count', check_positive_int),\n",
    "    ('num_hashtags', check_positive_int),\n",
    "    ('num_urls', check_positive_int),\n",
    "    ('num_mentions', check_positive_int),\n",
    "    ('created_at', check_date),\n",
    "    ('text', check_text),\n",
    "]\n",
    "\n",
    "# clean the dataset using validators ratio function.\n",
    "lg.info(\"Starting dataset cleaning with validators...\")\n",
    "deleted_rows = clean_invalid_rows(tweets, column_validators)\n",
    "lg.info(f\"Deleted rows {len(deleted_rows)} ({len(deleted_rows) / len(tweets)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to remove the `id` column because it's not relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dropping id column\n",
    "#\n",
    "tweets = tweets.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze all columns\n",
    "\n",
    "The followings cells perform analysis on type and convert invalid type in an OUTLIER_VALUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Examine the columns domain.\n",
    "#\n",
    "for col in tweets.columns:\n",
    "    if col == 'text':\n",
    "        #\n",
    "        # skip the text column\n",
    "        #\n",
    "        continue\n",
    "    lg.info(tweets[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a lot of invalid values, hence we need to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function that replaces invalid values with an outlier value\n",
    "def replace_with_outlier(dataset, col_name, check_function, outlier_value):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].parallel_map(check_function)\n",
    "    record_touched = len(v) - sum(v)\n",
    "    \n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: outlier_value)\n",
    "    return df, record_touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function for integer values\n",
    "def check_integer_column(x):\n",
    "    try:\n",
    "        # we try to cast to int\n",
    "        int(str(x))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all columns to be checked\n",
    "INTEGER_COLUMNS = [\n",
    "    'user_id',\n",
    "    'retweet_count',\n",
    "    'reply_count',\n",
    "    'favorite_count',\n",
    "    'num_hashtags',\n",
    "    'num_urls',\n",
    "    'num_mentions',\n",
    "]\n",
    "\n",
    "# outlier value\n",
    "OUTLIER_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replace invalid integer columns\n",
    "#\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, removed = replace_with_outlier(\n",
    "        tweets,\n",
    "        col,\n",
    "        check_integer_column,\n",
    "        OUTLIER_VALUE,\n",
    "    )\n",
    "    lg.info(f\"Detected {removed} {col} with invalid value, i.e. {removed / len(tweets) * 100}% of dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical columns, replace with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple function that replaces missing values with the median (only numerical)\n",
    "def clean_with_median(dataset, col_name):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].parallel_map(lambda x: x != OUTLIER_VALUE)\n",
    "    median = df[v == True][col_name].median()\n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: median)\n",
    "    \n",
    "    return df, sum(~v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing data with median\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, affected = clean_with_median(tweets, col)\n",
    "    lg.info(f'Detected {affected} rows with outlier value i.e. {affected / len(tweets) * 100}% of dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the text column, we want to make the column a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute statistics\n",
    "#\n",
    "invalid_texts = tweets['text'].parallel_map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_texts)} records, i.e. {sum(invalid_texts) / len(invalid_texts) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the text record\n",
    "def handle_text_record(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    else:\n",
    "        x = str(x).strip()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function\n",
    "tweets['text'] = tweets['text'].parallel_map(handle_text_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Managing the Datetime column\n",
    "#\n",
    "tweets = filter_datetime(tweets, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "tweets = tweets.astype({\n",
    "    'user_id': 'int64',\n",
    "    'retweet_count': 'int64',\n",
    "    'reply_count': 'int64',\n",
    "    'favorite_count': 'int64',\n",
    "    'num_hashtags': 'int64',\n",
    "    'num_urls': 'int64',\n",
    "    'num_mentions': 'int64',\n",
    "    'created_at': 'datetime64[ns]',\n",
    "    'text': 'string',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Printing statistics about cleaning\n",
    "#\n",
    "\n",
    "initial_ds_len = len(tweets)\n",
    "tweets = tweets.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(tweets)} duplicates record that are {(initial_ds_len - len(tweets)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "tweets[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Save the CSV\n",
    "#\n",
    "users.to_csv('cleaned_dataset/users_cleaned.csv')\n",
    "tweets.to_csv('cleaned_dataset/tweets_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution analysis\n",
    "\n",
    "In the following cell we plot statistics acoording to cleaned data to detect outliers and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Load the CSV\n",
    "#\n",
    "users = pd.read_csv('cleaned_dataset/users_cleaned.csv')\n",
    "tweets = pd.read_csv('cleaned_dataset/tweets_cleaned.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import build_grid_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['statuses_count'],\n",
    "        'title': 'Statues Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['bot'].map(lambda v: 'Bot' if v else 'User'),\n",
    "        'title': 'Bot and User Counts',\n",
    "        'rotation': True,\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['lang'],\n",
    "        'title': 'Languages Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['created_at'][users['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "        'title': 'User Creation Date Distribution',\n",
    "        'yscale': 'log',\n",
    "    }\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist_tweets():\n",
    "    configs = [\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['retweet_count'],\n",
    "            'title': 'Retweet Counts',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['reply_count'],\n",
    "            'title': 'Replay Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['favorite_count'],\n",
    "            'title': 'Favorite Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_hashtags'],\n",
    "            'title': 'Hashtag Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_urls'],\n",
    "            'title': 'Url Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_mentions'],\n",
    "            'title': 'Mentions Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['created_at'][tweets['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "            'title': 'Tweets Creation Date Distribution',\n",
    "            'yscale': 'log',\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    build_grid_plot(configs=configs)\n",
    "\n",
    "plot_hist_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all numerical columns there are outliers to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def boxplot_tweets_show(yscale=None):\n",
    "    configs = [\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['retweet_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['reply_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['favorite_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_hashtags'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_urls'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_mentions'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    build_grid_plot(configs=configs)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def replace_outliers(df, column_name, threshold):\n",
    "    column = df[column_name]\n",
    "    to_replace = len(column[column > threshold])\n",
    "    perc_to_replace = to_replace / len(column) * 100\n",
    "    lg.info(f'{to_replace} ({perc_to_replace}%) element replaced for column {column_name}')\n",
    "    median = column.median()\n",
    "    df[column_name] = column.parallel_map(lambda x: median if x > threshold else x)\n",
    "\n",
    "replace_outliers(tweets, 'retweet_count', 6e5)\n",
    "replace_outliers(tweets, 'reply_count', 6e4)\n",
    "replace_outliers(tweets, 'favorite_count', 1.2e5)\n",
    "replace_outliers(tweets, 'num_hashtags', 1e4)\n",
    "replace_outliers(tweets, 'num_urls', 1e4)\n",
    "replace_outliers(tweets, 'num_mentions', 1e5)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['statuses_count', 'bot']\n",
    "users.astype({'bot': 'int64'}).corr(method='pearson', numeric_only=True).loc[cols, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']\n",
    "tweets.corr(method='pearson', numeric_only=True).loc[cols, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ???? scatterplot pairwise\n",
    "plt.figure(figsize=(20, 10))\n",
    "tweets.plot.scatter(x='reply_count', y='favorite_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricipal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> TODO prof recap points for DATA UNDERSTANDING\n",
    "(last slide of data understanding)\n",
    "Checklist for Data Understanding\n",
    "- Determine the quality of the data.(e.g.syntactic accuracy)\n",
    "- Find outliers. (e. g. using visualization techniques)\n",
    "- Detect and examine missing values. Possible hidden by default values.\n",
    "- Discover new or confirm expected dependencies or correlations between attributes.\n",
    "- Check specific application dependent assumptions (e.g. the attribute follows a normal distribution)\n",
    "- Compare statistics with the expected behaviour.\n",
    "-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
