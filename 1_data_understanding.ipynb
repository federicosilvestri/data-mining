{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM - Data understanding [TASK 1.1]\n",
    "\n",
    "Exploring the dataset with analytical tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN Only with COLAB\n",
    "\n",
    "This cell will setup notebook for running on Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://FedericoSilvestri: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@github.com/federicosilvestri/data-mining.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd data-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10583/36680621.py:12: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import sys\n",
    "import logging as lg\n",
    "\n",
    "root = lg.getLogger()\n",
    "root.setLevel(lg.INFO)\n",
    "\n",
    "handler = lg.StreamHandler(sys.stdout)\n",
    "handler.setLevel(lg.DEBUG)\n",
    "formatter = lg.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\n",
    "pandarallel.initialize(\n",
    "    progress_bar=True,\n",
    "    use_memory_fs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Fetching the dataset using our native python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 09:55:34,664 - root - INFO - Pandas reading dataset tweets.csv...\n",
      "2022-11-01 09:55:34,664 - root - INFO - Pandas reading dataset tweets.csv...\n",
      "2022-11-01 09:56:04,911 - root - INFO - Pandas reading dataset users.csv...\n",
      "2022-11-01 09:56:04,911 - root - INFO - Pandas reading dataset users.csv...\n"
     ]
    }
   ],
   "source": [
    "from utils import fetch_dataset\n",
    "\n",
    "dataset = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n",
    "\n",
    "Show `users.csv` information: types of data and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11508 entries, 0 to 11507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              11508 non-null  int64  \n",
      " 1   name            11507 non-null  object \n",
      " 2   lang            11508 non-null  object \n",
      " 3   bot             11508 non-null  int64  \n",
      " 4   created_at      11508 non-null  object \n",
      " 5   statuses_count  11109 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 539.6+ KB\n"
     ]
    }
   ],
   "source": [
    "users = dataset['users.csv'].copy() # make a copy\n",
    "\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en                    9970\n",
       "it                     906\n",
       "es                     319\n",
       "pt                      65\n",
       "en-gb                   50\n",
       "ru                      42\n",
       "fr                      36\n",
       "ja                      33\n",
       "zh-tw                   17\n",
       "tr                      14\n",
       "id                      12\n",
       "ko                       9\n",
       "de                       8\n",
       "nl                       6\n",
       "en-GB                    4\n",
       "ar                       3\n",
       "zh-TW                    3\n",
       "da                       2\n",
       "Select Language...       2\n",
       "en-AU                    1\n",
       "zh-cn                    1\n",
       "pl                       1\n",
       "el                       1\n",
       "fil                      1\n",
       "sv                       1\n",
       "xx-lc                    1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display lang values\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have:\n",
    "\n",
    "1. `xx-lc`\n",
    "2. `Select Language...`\n",
    "\n",
    "That are not a valid language.\n",
    "We have decided to use iso639-1 Python library to detect valid languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6116\n",
       "0    5392\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display BOT values\n",
    "# 0 -> it's a human!\n",
    "# 1 -> it's a bot!\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have clean data for `bot` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets\n",
    "\n",
    "Show `tweets.csv information: types of data and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13664696 entries, 0 to 13664695\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   id              object\n",
      " 1   user_id         object\n",
      " 2   retweet_count   object\n",
      " 3   reply_count     object\n",
      " 4   favorite_count  object\n",
      " 5   num_hashtags    object\n",
      " 6   num_urls        object\n",
      " 7   num_mentions    object\n",
      " 8   created_at      object\n",
      " 9   text            object\n",
      "dtypes: object(10)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "tweets = dataset['tweets.csv'].copy()\n",
    "\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality assessment and data cleaning\n",
    "\n",
    "In these cells we are going to understand and clean the data of two datasets.\n",
    "The analysis performs:\n",
    "\n",
    "1. Replacement of null values with median if type is numerical, mode if type is categorical and **outlier** timestamp value if type is datetime.\n",
    "2. Deletion of rows that has a ratio between valid values and invalid values `< k` where `k` is a param with default value 60%.\n",
    "3. Understand and replace categorical value based on their domain. For example, the language column contains invalid language codes, and we replace them with the mode value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definition for outlier values\n",
    "min_date = pd.Timestamp('2006-03-21') # the date when Twitter has started the activity.\n",
    "max_date = pd.Timestamp('2022-09-28') # the date when dataset has been collected.\n",
    "\n",
    "# OUTLIER constants\n",
    "OUTLIER_TIMESTAMP = pd.Timestamp('1800-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_invalid_rows(df, column_validators, ratio=0.6):\n",
    "    #\n",
    "    # This function is a generic function, that performs cleaning of rows that are invalid.\n",
    "    # We define row as invalid if the ratio between valid and invalid attributes \n",
    "    # is greater than `ratio` parameter.\n",
    "    # \n",
    "    # The validation of single attribute is entrusted to the combination of \n",
    "    # lambda function named `validator` and the fact that the attribute is nan.\n",
    "    #\n",
    "    n_null_items = int(len(column_validators) * ratio)\n",
    "    \n",
    "    def check_invalid_rows_callback(row):\n",
    "        count = 0\n",
    "        for head, validator in column_validators:\n",
    "            value = row[head]\n",
    "            if pd.isnull(value) or (validator is not None and validator(value)):\n",
    "                count += 1\n",
    "        return count > n_null_items\n",
    "\n",
    "    outliers = df.parallel_apply(check_invalid_rows_callback, axis=1)\n",
    "    return df[~outliers], sum(outliers)\n",
    "\n",
    "# Definition of generic lambda validator functions\n",
    "check_int = lambda label: not bool(re.search(r'^(\\d)+(\\.0+)?$', str(label))) # checks, using regex if attribute is integer\n",
    "check_positive_int = lambda label: check_int(label) or float(label) < 0 # checks if label is positive\n",
    "check_date = lambda label: pd.Timestamp(label) < min_date or pd.Timestamp(label) > max_date # checks timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e20a501967d4d619ab56408c8a08b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2877), Label(value='0 / 2877'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 09:56:14,363 - root - INFO - Deleted rows 0 (0.0%)\n",
      "2022-11-01 09:56:14,363 - root - INFO - Deleted rows 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "from langcodes import tag_is_valid # import the library for ISO639-1 codes\n",
    "\n",
    "# For each column define a validator.\n",
    "column_validators = [\n",
    "    ('id', check_int),\n",
    "    ('name', None),\n",
    "    ('lang', tag_is_valid),\n",
    "    ('bot', lambda label: label == '1' or label == '0'),\n",
    "    ('statuses_count', check_int),\n",
    "    ('created_at', check_date),\n",
    "]\n",
    "\n",
    "#\n",
    "# Execute the cleaning function.\n",
    "#\n",
    "users, deleted_rows = clean_invalid_rows(users, column_validators)\n",
    "lg.info(f\"Deleted rows {deleted_rows} ({deleted_rows / len(users)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 09:56:16,576 - root - INFO - Found 1 records, i.e. 0.008689607229753215% of dataset\n",
      "2022-11-01 09:56:16,576 - root - INFO - Found 1 records, i.e. 0.008689607229753215% of dataset\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Replacement of invalid names\n",
    "#\n",
    "invalid_names = users['name'].map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_names)} records, i.e. {sum(invalid_names) / len(invalid_names) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6116\n",
       "0    5392\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Explore bot column\n",
    "#\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all values of column bot are 0,1 so we can convert it into boolean field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en       9973\n",
       "it        906\n",
       "es        319\n",
       "pt         65\n",
       "en-gb      54\n",
       "ru         42\n",
       "fr         36\n",
       "ja         33\n",
       "zh-tw      20\n",
       "tr         14\n",
       "id         12\n",
       "ko          9\n",
       "de          8\n",
       "nl          6\n",
       "ar          3\n",
       "da          2\n",
       "en-au       1\n",
       "zh-cn       1\n",
       "pl          1\n",
       "el          1\n",
       "fil         1\n",
       "sv          1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Replacement of invalid languages\n",
    "#\n",
    "\n",
    "# first normalize to lower case all langs\n",
    "users['lang'] = users['lang'].str.lower()\n",
    "\n",
    "# calculate the mode for this categorical value\n",
    "user_lang_mode = users['lang'].mode()[0]\n",
    "\n",
    "# lambda function for substition\n",
    "lang_subst_lambda = lambda x: x if tag_is_valid(x) else user_lang_mode\n",
    "\n",
    "# execute substitution\n",
    "users['lang'] = users['lang'].map(lang_subst_lambda)\n",
    "\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a constant that marks an attribute as an outlier.\n",
    "#\n",
    "\n",
    "def filter_datetime(df, att):\n",
    "    def parse_and_check_datetime(el):\n",
    "        try:\n",
    "            datetime = pd.Timestamp(el) # parse datetime as Timestamp\n",
    "            # checks validity\n",
    "            if datetime < min_date or datetime > max_date:\n",
    "                # is an outlier\n",
    "                return OUTLIER_TIMESTAMP\n",
    "            else:\n",
    "                # is not an outlier\n",
    "                return datetime\n",
    "        except ValueError:\n",
    "            # cannot parse as timestamp, it's an outlier\n",
    "            return OUTLIER_TIMESTAMP\n",
    "    df[att] = df[att].parallel_map(parse_and_check_datetime)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff12a5cd3f04149acce68d220202986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2877), Label(value='0 / 2877'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the filter to datetime column\n",
    "users = filter_datetime(users, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Handling the statuses_count column.\n",
    "#\n",
    "status_count_median = users['statuses_count'].median()\n",
    "\n",
    "# replace the null values with median\n",
    "users['statuses_count'].fillna(status_count_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "users = users.astype({\n",
    "    'id': 'int64',\n",
    "    'name': 'string',\n",
    "    'lang': 'string',\n",
    "    'bot': 'bool',\n",
    "    'statuses_count': 'int64',\n",
    "    'created_at': 'datetime64[ns]'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 09:56:24,362 - root - INFO - Removed 0 duplicates record that are 0.0% of dataset.\n",
      "2022-11-01 09:56:24,362 - root - INFO - Removed 0 duplicates record that are 0.0% of dataset.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Removing duplicate records.\n",
    "#\n",
    "initial_ds_len = len(users)\n",
    "users = users.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(users)} duplicates record that are {(initial_ds_len - len(users)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 0 to 11507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              11508 non-null  int64         \n",
      " 1   name            11507 non-null  string        \n",
      " 2   lang            11508 non-null  string        \n",
      " 3   bot             11508 non-null  bool          \n",
      " 4   created_at      11508 non-null  datetime64[ns]\n",
      " 5   statuses_count  11508 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), string(2)\n",
      "memory usage: 550.7 KB\n"
     ]
    }
   ],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the columns are now validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "      <th>bot</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11507</td>\n",
       "      <td>11508</td>\n",
       "      <td>11508</td>\n",
       "      <td>11508.000000</td>\n",
       "      <td>11508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11360</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sara</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7</td>\n",
       "      <td>9973</td>\n",
       "      <td>6116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5681.686566</td>\n",
       "      <td>2017-10-03 21:23:16.013121280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2012-01-24 01:57:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2017-01-18 09:50:16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>2018-01-30 17:20:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2520.250000</td>\n",
       "      <td>2019-02-25 00:17:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>399555.000000</td>\n",
       "      <td>2020-04-21 07:28:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18769.594489</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name   lang    bot  statuses_count                     created_at\n",
       "count   11507  11508  11508    11508.000000                          11508\n",
       "unique  11360     22      2             NaN                            NaN\n",
       "top      Sara     en   True             NaN                            NaN\n",
       "freq        7   9973   6116             NaN                            NaN\n",
       "mean      NaN    NaN    NaN     5681.686566  2017-10-03 21:23:16.013121280\n",
       "min       NaN    NaN    NaN        0.000000            2012-01-24 01:57:38\n",
       "25%       NaN    NaN    NaN       42.000000     2017-01-18 09:50:16.500000\n",
       "50%       NaN    NaN    NaN       68.000000            2018-01-30 17:20:36\n",
       "75%       NaN    NaN    NaN     2520.250000            2019-02-25 00:17:30\n",
       "max       NaN    NaN    NaN   399555.000000            2020-04-21 07:28:31\n",
       "std       NaN    NaN    NaN    18769.594489                            NaN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "users[['name', 'lang', 'bot', 'statuses_count', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a function for validating text of tweet.\n",
    "#\n",
    "check_text = lambda x: not x or len(str(x)) <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 09:56:29,077 - root - INFO - Starting dataset cleaning with validators...\n",
      "2022-11-01 09:56:29,077 - root - INFO - Starting dataset cleaning with validators...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1f70d9129b4becab4e51ea1fc6bc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3416174), Label(value='0 / 3416174…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_validators = [\n",
    "    ('id', check_positive_int),\n",
    "    ('user_id', check_positive_int),\n",
    "    ('retweet_count', check_positive_int),\n",
    "    ('reply_count', check_positive_int),\n",
    "    ('favorite_count', check_positive_int),\n",
    "    ('num_hashtags', check_positive_int),\n",
    "    ('num_urls', check_positive_int),\n",
    "    ('num_mentions', check_positive_int),\n",
    "    ('created_at', check_date),\n",
    "    ('text', check_text),\n",
    "]\n",
    "\n",
    "# clean the dataset using validators ratio function.\n",
    "lg.info(\"Starting dataset cleaning with validators...\")\n",
    "deleted_rows = clean_invalid_rows(tweets, column_validators)\n",
    "lg.info(f\"Deleted rows {len(deleted_rows)} ({len(deleted_rows) / len(tweets)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to remove the `id` column because it's not relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dropping id column\n",
    "#\n",
    "tweets = tweets.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze all columns\n",
    "\n",
    "The followings cells perform analysis on type and convert invalid type in an OUTLIER_VALUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Examine the columns domain.\n",
    "#\n",
    "for col in tweets.columns:\n",
    "    if col == 'text':\n",
    "        #\n",
    "        # skip the text column\n",
    "        #\n",
    "        continue\n",
    "    lg.info(tweets[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a lot of invalid values, hence we need to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function that replaces invalid values with an outlier value\n",
    "def replace_with_outlier(dataset, col_name, check_function, outlier_value):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].parallel_map(check_function)\n",
    "    record_touched = len(v) - sum(v)\n",
    "    \n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: outlier_value)\n",
    "    return df, record_touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function for integer values\n",
    "def check_integer_column(x):\n",
    "    try:\n",
    "        # we try to cast to int\n",
    "        int(str(x))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all columns to be checked\n",
    "INTEGER_COLUMNS = [\n",
    "    'user_id',\n",
    "    'retweet_count',\n",
    "    'reply_count',\n",
    "    'favorite_count',\n",
    "    'num_hashtags',\n",
    "    'num_urls',\n",
    "    'num_mentions',\n",
    "]\n",
    "\n",
    "# outlier value\n",
    "OUTLIER_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replace invalid integer columns\n",
    "#\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, removed = replace_with_outlier(\n",
    "        tweets,\n",
    "        col,\n",
    "        check_integer_column,\n",
    "        OUTLIER_VALUE,\n",
    "    )\n",
    "    lg.info(f\"Detected {removed} {col} with invalid value, i.e. {removed / len(tweets) * 100}% of dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical columns, replace with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple function that replaces missing values with the median (only numerical)\n",
    "def clean_with_median(dataset, col_name):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].map(lambda x: x != OUTLIER_VALUE)\n",
    "    median = df[v == True][col_name].median()\n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: median)\n",
    "    \n",
    "    return df, sum(~v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing data with median\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, affected = clean_with_median(tweets, col)\n",
    "    lg.info(f'Detected {affected} rows with outlier value i.e. {affected / len(tweets) * 100}% of dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the text column, we want to make the column a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute statistics\n",
    "#\n",
    "invalid_texts = tweets['text'].map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_texts)} records, i.e. {sum(invalid_texts) / len(invalid_texts) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the text record\n",
    "def handle_text_record(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function\n",
    "tweets['text'] = tweets['text'].map(handle_text_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Managing the Datetime column\n",
    "#\n",
    "tweets = filter_datetime(tweets, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "tweets = tweets.astype({\n",
    "    'user_id': 'int64',\n",
    "    'retweet_count': 'int64',\n",
    "    'reply_count': 'int64',\n",
    "    'favorite_count': 'int64',\n",
    "    'num_hashtags': 'int64',\n",
    "    'num_urls': 'int64',\n",
    "    'num_mentions': 'int64',\n",
    "    'created_at': 'datetime64[ns]',\n",
    "    'text': 'string',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Printing statistics about cleaning\n",
    "#\n",
    "\n",
    "initial_ds_len = len(tweets)\n",
    "tweets = tweets.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(tweets)} duplicates record that are {(initial_ds_len - len(tweets)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "tweets[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Save the CSV\n",
    "#\n",
    "tweets.to_csv('cleaned_dataset/tweets_cleaned.csv')\n",
    "users.to_csv('cleaned_dataset/users_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution analysis\n",
    "\n",
    "In the following cell we plot statistics acoording to cleaned data to detect outliers and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Load the CSV\n",
    "#\n",
    "users = pd.read_csv('cleaned_dataset/users_cleaned.csv')\n",
    "tweets = pd.read_csv('cleaned_dataset/tweets_cleaned.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.astype({\n",
    "    'user_id': 'int64',\n",
    "    'retweet_count': 'int64',\n",
    "    'reply_count': 'int64',\n",
    "    'favorite_count': 'int64',\n",
    "    'num_hashtags': 'int64',\n",
    "    'num_urls': 'int64',\n",
    "    'num_mentions': 'int64',\n",
    "    'created_at': 'datetime64[ns]',\n",
    "    'text': 'string',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import build_grid_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['statuses_count'],\n",
    "        'title': 'Statues Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['bot'].map(lambda v: 'Bot' if v else 'User'),\n",
    "        'title': 'Bot and User Counts',\n",
    "        'rotation': True,\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['lang'],\n",
    "        'title': 'Languages Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['created_at'][users['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "        'title': 'User Creation Date Distribution',\n",
    "        'yscale': 'log',\n",
    "    }\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['retweet_count'],\n",
    "        'title': 'Retweet Counts',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['reply_count'],\n",
    "        'title': 'Replay Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['favorite_count'],\n",
    "        'title': 'Favorite Counts',\n",
    "        'yscale': 'log',\n",
    "        'ylim_inf': 0,\n",
    "        'ylim_sup': 1e9,\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['num_hashtags'],\n",
    "        'title': 'Hashtag Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['num_urls'],\n",
    "        'title': 'Url Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['num_mentions'],\n",
    "        'title': 'Mentions Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': tweets['created_at'][tweets['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "        'title': 'Tweets Creation Date Distribution',\n",
    "        'yscale': 'log',\n",
    "    }\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all numerical columns there are outliers to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def boxplot_tweets_show():\n",
    "    configs = [\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['retweet_count']\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['reply_count']\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['favorite_count']\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_hashtags']\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_urls']\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_mentions']\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    build_grid_plot(configs=configs)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def replace_outliers(df, column_name, threshold):\n",
    "    column = df[column_name]\n",
    "    to_replace = len(column[column > threshold])\n",
    "    perc_to_replace = to_replace / len(column) * 100\n",
    "    lg.info(f'{to_replace} ({perc_to_replace}%) element replaced for column {column_name}')\n",
    "    median = column.median()\n",
    "    df[column_name] = column.map(lambda x: median if x > threshold else x)\n",
    "\n",
    "replace_outliers(tweets, 'retweet_count', 6e5)\n",
    "replace_outliers(tweets, 'reply_count', 6e4)\n",
    "replace_outliers(tweets, 'favorite_count', 1.2e5)\n",
    "replace_outliers(tweets, 'num_hashtags', 1e4)\n",
    "replace_outliers(tweets, 'num_urls', 1e4)\n",
    "replace_outliers(tweets, 'num_mentions', 1e5)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['statuses_count', 'bot']\n",
    "users.astype({'bot': 'int64'}).corr(method='pearson', numeric_only=True).loc[cols, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']\n",
    "tweets.corr(method='pearson', numeric_only=True).loc[cols, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ???? scatterplot\n",
    "plt.figure(figsize=(20, 10))\n",
    "tweets.plot.scatter(x='reply_count', y='favorite_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> TODO prof recap points for DATA UNDERSTANDING\n",
    "(last slide of data understanding)\n",
    "Checklist for Data Understanding\n",
    "- Determine the quality of the data.(e.g.syntactic accuracy)\n",
    "- Find outliers. (e. g. using visualization techniques)\n",
    "- Detect and examine missing values. Possible hidden by default values.\n",
    "- Discover new or confirm expected dependencies or correlations between attributes.\n",
    "- Check specific application dependent assumptions (e.g. the attribute follows a normal distribution)\n",
    "- Compare statistics with the expected behaviour.\n",
    "-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
