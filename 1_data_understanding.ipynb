{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM - Data understanding [TASK 1.1]\n",
    "\n",
    "Exploring the dataset with analytical tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN Only with COLAB\n",
    "\n",
    "This cell will setup notebook for running on Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://FedericoSilvestri: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@github.com/federicosilvestri/data-mining.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd data-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6295/4042758330.py:12: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import sys\n",
    "import logging as lg\n",
    "import os\n",
    "\n",
    "root = lg.getLogger()\n",
    "root.setLevel(lg.INFO)\n",
    "\n",
    "\n",
    "nb_workers = int(os.cpu_count() / 2 + 1)\n",
    "\n",
    "handler = lg.StreamHandler(sys.stdout)\n",
    "handler.setLevel(lg.DEBUG)\n",
    "formatter = lg.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "pandarallel.initialize(\n",
    "    progress_bar=True,\n",
    "    nb_workers=nb_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Fetching the dataset using our native python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-02 20:05:25,137 - root - INFO - Pandas reading dataset tweets.csv...\n",
      "2022-11-02 20:05:56,188 - root - INFO - Pandas reading dataset users.csv...\n"
     ]
    }
   ],
   "source": [
    "from utils import fetch_dataset\n",
    "\n",
    "dataset = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n",
    "\n",
    "Show `users.csv` information: types of data and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11508 entries, 0 to 11507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              11508 non-null  int64  \n",
      " 1   name            11507 non-null  object \n",
      " 2   lang            11508 non-null  object \n",
      " 3   bot             11508 non-null  int64  \n",
      " 4   created_at      11508 non-null  object \n",
      " 5   statuses_count  11109 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 539.6+ KB\n"
     ]
    }
   ],
   "source": [
    "users = dataset['users.csv'].copy() # make a copy\n",
    "\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en                    9970\n",
       "it                     906\n",
       "es                     319\n",
       "pt                      65\n",
       "en-gb                   50\n",
       "ru                      42\n",
       "fr                      36\n",
       "ja                      33\n",
       "zh-tw                   17\n",
       "tr                      14\n",
       "id                      12\n",
       "ko                       9\n",
       "de                       8\n",
       "nl                       6\n",
       "en-GB                    4\n",
       "ar                       3\n",
       "zh-TW                    3\n",
       "da                       2\n",
       "Select Language...       2\n",
       "en-AU                    1\n",
       "zh-cn                    1\n",
       "pl                       1\n",
       "el                       1\n",
       "fil                      1\n",
       "sv                       1\n",
       "xx-lc                    1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display lang values\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have:\n",
    "\n",
    "1. `xx-lc`\n",
    "2. `Select Language...`\n",
    "\n",
    "That are not a valid language.\n",
    "We have decided to use iso639-1 Python library to detect valid languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6116\n",
       "0    5392\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display BOT values\n",
    "# 0 -> it's a human!\n",
    "# 1 -> it's a bot!\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have clean data for `bot` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets\n",
    "\n",
    "Show `tweets.csv information: types of data and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13664696 entries, 0 to 13664695\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   id              object\n",
      " 1   user_id         object\n",
      " 2   retweet_count   object\n",
      " 3   reply_count     object\n",
      " 4   favorite_count  object\n",
      " 5   num_hashtags    object\n",
      " 6   num_urls        object\n",
      " 7   num_mentions    object\n",
      " 8   created_at      object\n",
      " 9   text            object\n",
      "dtypes: object(10)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "tweets = dataset['tweets.csv'].copy()\n",
    "\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality assessment and data cleaning\n",
    "\n",
    "In these cells we are going to understand and clean the data of two datasets.\n",
    "The analysis performs:\n",
    "\n",
    "1. Replacement of null values with median if type is numerical, mode if type is categorical and **outlier** timestamp value if type is datetime.\n",
    "2. Deletion of rows that has a ratio between valid values and invalid values `< k` where `k` is a param with default value 60%.\n",
    "3. Understand and replace categorical value based on their domain. For example, the language column contains invalid language codes, and we replace them with the mode value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definition for outlier values\n",
    "min_date = pd.Timestamp('2006-03-21') # the date when Twitter has started the activity.\n",
    "max_date = pd.Timestamp('2022-09-28') # the date when dataset has been collected.\n",
    "\n",
    "# OUTLIER constants\n",
    "OUTLIER_TIMESTAMP = pd.Timestamp('1800-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_invalid_rows(df, column_validators, ratio=0.3):\n",
    "    #\n",
    "    # This function is a generic function, that performs cleaning of rows that are invalid.\n",
    "    # We define row as invalid if the ratio between valid and invalid attributes \n",
    "    # is greater than `ratio` parameter.\n",
    "    # \n",
    "    # The validation of single attribute is entrusted to the combination of \n",
    "    # lambda function named `validator` and the fact that the attribute is nan.\n",
    "    #\n",
    "    def check_invalid_rows_callback(row) -> bool:\n",
    "        # this function must return True if record is to be deleted.\n",
    "        invalid_count = 0\n",
    "        \n",
    "        for head, validator in column_validators:\n",
    "            value = row[head]\n",
    "            if pd.isnull(value):\n",
    "                invalid_count += 1\n",
    "            else:\n",
    "                if validator is not None and not validator(value):\n",
    "                    invalid_count += 1\n",
    "        return float(invalid_count / len(column_validators)) < ratio\n",
    "\n",
    "    trash = df.parallel_apply(check_invalid_rows_callback, axis=1)\n",
    "    return df[~trash], sum(trash)\n",
    "\n",
    "\n",
    "def check_int(label): # checks, using regex if attribute is integer\n",
    "    try:\n",
    "        int(str(label))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "check_positive_int = lambda label: check_int(label) and float(label) >= 0 # checks if label is positive\n",
    "check_date = lambda label: min_date < pd.Timestamp(label) < max_date # checks timestamps\n",
    "check_bot = lambda x: int(x) in [0, 1] # check bot\n",
    "check_text = lambda x: len(str(x)) > 0 # check texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38a9cd881a4b09b15bd5d1699c68e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2302), Label(value='0 / 2302'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-02 20:05:58,334 - root - INFO - Deleted rows 11504 (2876.0%)\n"
     ]
    }
   ],
   "source": [
    "from langcodes import tag_is_valid # import the library for ISO639-1 codes\n",
    "\n",
    "# For each column define a validator.\n",
    "column_validators = [\n",
    "    ('id', check_int),\n",
    "    ('name', check_text),\n",
    "    ('lang', tag_is_valid),\n",
    "    ('bot', check_bot),\n",
    "    ('statuses_count', check_int),\n",
    "    ('created_at', check_date),\n",
    "]\n",
    "\n",
    "#\n",
    "# Execute the cleaning function.\n",
    "#\n",
    "users, deleted_rows = clean_invalid_rows(users, column_validators)\n",
    "lg.info(f\"Deleted rows {deleted_rows} ({deleted_rows / len(users)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-02 20:05:58,341 - root - INFO - Found 1 records, i.e. 25.0% of dataset\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Replacement of invalid names\n",
    "#\n",
    "invalid_names = users['name'].map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_names)} records, i.e. {sum(invalid_names) / len(invalid_names) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Explore bot column\n",
    "#\n",
    "users['bot'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandarallel/data_types/series.py:42: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  yield data[chunk_]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f25f3926e44ab4825c90369293a545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Clean missing user names\n",
    "#\n",
    "users['name'] = users['name'].parallel_map(lambda t: '' if pd.isnull(t) else t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all values of column bot are 0,1 so we can convert it into boolean field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "select language...    3\n",
       "en                    1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Replacement of invalid languages\n",
    "#\n",
    "\n",
    "# first normalize to lower case all langs\n",
    "users['lang'] = users['lang'].str.lower()\n",
    "\n",
    "# calculate the mode for this categorical value\n",
    "user_lang_mode = users['lang'].mode()[0]\n",
    "\n",
    "# lambda function for substition\n",
    "lang_subst_lambda = lambda x: x if tag_is_valid(x) else user_lang_mode\n",
    "\n",
    "# execute substitution\n",
    "users['lang'] = users['lang'].map(lang_subst_lambda)\n",
    "\n",
    "users['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a constant that marks an attribute as an outlier.\n",
    "#\n",
    "\n",
    "def filter_datetime(df, att):\n",
    "    def parse_and_check_datetime(el):\n",
    "        try:\n",
    "            datetime = pd.Timestamp(el) # parse datetime as Timestamp\n",
    "            # checks validity\n",
    "            if datetime < min_date or datetime > max_date:\n",
    "                # is an outlier\n",
    "                return OUTLIER_TIMESTAMP\n",
    "            else:\n",
    "                # is not an outlier\n",
    "                return datetime\n",
    "        except ValueError:\n",
    "            # cannot parse as timestamp, it's an outlier\n",
    "            return OUTLIER_TIMESTAMP\n",
    "    df[att] = df[att].parallel_map(parse_and_check_datetime)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandarallel/data_types/series.py:42: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  yield data[chunk_]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ea7e6269474f6d8e70e5c8d02df437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the filter to datetime column\n",
    "users = filter_datetime(users, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Handling the statuses_count column.\n",
    "#\n",
    "status_count_median = users['statuses_count'].median()\n",
    "\n",
    "# replace the null values with median\n",
    "users['statuses_count'].fillna(status_count_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "users = users.astype({\n",
    "    'id': 'int64',\n",
    "    'name': 'string',\n",
    "    'lang': 'string',\n",
    "    'bot': 'bool',\n",
    "    'statuses_count': 'int64',\n",
    "    'created_at': 'datetime64[ns]'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-02 20:05:59,628 - root - INFO - Removed 0 duplicates record that are 0.0% of dataset.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Removing duplicate records.\n",
    "#\n",
    "initial_ds_len = len(users)\n",
    "users = users.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(users)} duplicates record that are {(initial_ds_len - len(users)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4 entries, 1535 to 10065\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              4 non-null      int64         \n",
      " 1   name            4 non-null      string        \n",
      " 2   lang            4 non-null      string        \n",
      " 3   bot             4 non-null      bool          \n",
      " 4   created_at      4 non-null      datetime64[ns]\n",
      " 5   statuses_count  4 non-null      int64         \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), string(2)\n",
      "memory usage: 196.0 bytes\n"
     ]
    }
   ],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the columns are now validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "      <th>bot</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td>select language...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5526.500000</td>\n",
       "      <td>2018-04-02 20:32:30.250000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2014-04-09 21:36:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>2017-09-11 10:23:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3325.500000</td>\n",
       "      <td>2019-05-13 18:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8774.000000</td>\n",
       "      <td>2019-12-03 04:21:04.249999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15398.000000</td>\n",
       "      <td>2020-01-05 00:09:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7258.384233</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                lang    bot  statuses_count  \\\n",
       "count     4                   4      4        4.000000   \n",
       "unique    4                   2      1             NaN   \n",
       "top          select language...  False             NaN   \n",
       "freq      1                   3      4             NaN   \n",
       "mean    NaN                 NaN    NaN     5526.500000   \n",
       "min     NaN                 NaN    NaN       57.000000   \n",
       "25%     NaN                 NaN    NaN       78.000000   \n",
       "50%     NaN                 NaN    NaN     3325.500000   \n",
       "75%     NaN                 NaN    NaN     8774.000000   \n",
       "max     NaN                 NaN    NaN    15398.000000   \n",
       "std     NaN                 NaN    NaN     7258.384233   \n",
       "\n",
       "                           created_at  \n",
       "count                               4  \n",
       "unique                            NaN  \n",
       "top                               NaN  \n",
       "freq                              NaN  \n",
       "mean    2018-04-02 20:32:30.250000128  \n",
       "min               2014-04-09 21:36:02  \n",
       "25%               2017-09-11 10:23:26  \n",
       "50%               2019-05-13 18:12:00  \n",
       "75%     2019-12-03 04:21:04.249999872  \n",
       "max               2020-01-05 00:09:59  \n",
       "std                               NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "users[['name', 'lang', 'bot', 'statuses_count', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-02 20:05:59,684 - root - INFO - Starting dataset cleaning with validators...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919f7802ce65498d956248cb87091c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2732940), Label(value='0 / 2732940…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_validators = [\n",
    "    ('id', check_positive_int),\n",
    "    ('user_id', check_positive_int),\n",
    "    ('retweet_count', check_positive_int),\n",
    "    ('reply_count', check_positive_int),\n",
    "    ('favorite_count', check_positive_int),\n",
    "    ('num_hashtags', check_positive_int),\n",
    "    ('num_urls', check_positive_int),\n",
    "    ('num_mentions', check_positive_int),\n",
    "    ('created_at', check_date),\n",
    "    ('text', check_text),\n",
    "]\n",
    "\n",
    "# clean the dataset using validators ratio function.\n",
    "lg.info(\"Starting dataset cleaning with validators...\")\n",
    "deleted_rows = clean_invalid_rows(tweets, column_validators)\n",
    "lg.info(f\"Deleted rows {len(deleted_rows)} ({len(deleted_rows) / len(tweets)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to remove the `id` column because it's not relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dropping id column\n",
    "#\n",
    "tweets = tweets.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze all columns\n",
    "\n",
    "The followings cells perform analysis on type and convert invalid type in an OUTLIER_VALUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Examine the columns domain.\n",
    "#\n",
    "for col in tweets.columns:\n",
    "    if col == 'text':\n",
    "        #\n",
    "        # skip the text column\n",
    "        #\n",
    "        continue\n",
    "    lg.info(tweets[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a lot of invalid values, hence we need to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function that replaces invalid values with an outlier value\n",
    "def replace_with_outlier(dataset, col_name, check_function, outlier_value):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].parallel_map(check_function)\n",
    "    record_touched = len(v) - sum(v)\n",
    "    \n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: outlier_value)\n",
    "    return df, record_touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function for integer values\n",
    "def check_integer_column(x):\n",
    "    try:\n",
    "        # we try to cast to int\n",
    "        int(str(x))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all columns to be checked\n",
    "INTEGER_COLUMNS = [\n",
    "    'user_id',\n",
    "    'retweet_count',\n",
    "    'reply_count',\n",
    "    'favorite_count',\n",
    "    'num_hashtags',\n",
    "    'num_urls',\n",
    "    'num_mentions',\n",
    "]\n",
    "\n",
    "# outlier value\n",
    "OUTLIER_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replace invalid integer columns\n",
    "#\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, removed = replace_with_outlier(\n",
    "        tweets,\n",
    "        col,\n",
    "        check_integer_column,\n",
    "        OUTLIER_VALUE,\n",
    "    )\n",
    "    lg.info(f\"Detected {removed} {col} with invalid value, i.e. {removed / len(tweets) * 100}% of dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical columns, replace with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple function that replaces missing values with the median (only numerical)\n",
    "def clean_with_median(dataset, col_name):\n",
    "    df = dataset.copy()\n",
    "    v = df[col_name].parallel_map(lambda x: x != OUTLIER_VALUE)\n",
    "    median = df[v == True][col_name].median()\n",
    "    df.loc[v == False, col_name] = df[v == False][col_name].apply(lambda x: median)\n",
    "    \n",
    "    return df, sum(~v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing data with median\n",
    "for col in INTEGER_COLUMNS:\n",
    "    tweets, affected = clean_with_median(tweets, col)\n",
    "    lg.info(f'Detected {affected} rows with outlier value i.e. {affected / len(tweets) * 100}% of dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the text column, we want to make the column a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute statistics\n",
    "#\n",
    "invalid_texts = tweets['text'].parallel_map(pd.isnull)\n",
    "lg.info(f\"Found {sum(invalid_texts)} records, i.e. {sum(invalid_texts) / len(invalid_texts) * 100}% of dataset\")\n",
    "# to optize the memory\n",
    "del invalid_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the text record\n",
    "def handle_text_record(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    else:\n",
    "        x = str(x).strip()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function\n",
    "tweets['text'] = tweets['text'].parallel_map(handle_text_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Managing the Datetime column\n",
    "#\n",
    "tweets = filter_datetime(tweets, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Casting all dataset\n",
    "#\n",
    "tweets = tweets.astype({\n",
    "    'user_id': 'int64',\n",
    "    'retweet_count': 'int64',\n",
    "    'reply_count': 'int64',\n",
    "    'favorite_count': 'int64',\n",
    "    'num_hashtags': 'int64',\n",
    "    'num_urls': 'int64',\n",
    "    'num_mentions': 'int64',\n",
    "    'created_at': 'datetime64[ns]',\n",
    "    'text': 'string',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Printing statistics about cleaning\n",
    "#\n",
    "\n",
    "initial_ds_len = len(tweets)\n",
    "tweets = tweets.drop_duplicates()\n",
    "lg.info(f'Removed {initial_ds_len - len(tweets)} duplicates record that are {(initial_ds_len - len(tweets)) / initial_ds_len * 100}% of dataset.')\n",
    "del initial_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Describe the pre-processed dataset with all columns.\n",
    "#\n",
    "tweets[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'created_at']].describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution analysis\n",
    "\n",
    "In the following cell we plot statistics acoording to cleaned data to detect outliers and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Load the CSV\n",
    "#\n",
    "#users = pd.read_csv('cleaned_dataset/users_cleaned.csv')\n",
    "#tweets = pd.read_csv('cleaned_dataset/tweets_cleaned.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import build_grid_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['statuses_count'],\n",
    "        'title': 'Statues Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['bot'].map(lambda v: 'Bot' if v else 'User'),\n",
    "        'title': 'Bot and User Counts',\n",
    "        'rotation': True,\n",
    "    },\n",
    "    {\n",
    "        'type': 'bar',\n",
    "        'column': users['lang'],\n",
    "        'title': 'Languages Counts',\n",
    "        'yscale': 'log',\n",
    "    },\n",
    "    {\n",
    "        'type': 'hist',\n",
    "        'column': users['created_at'][users['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "        'title': 'User Creation Date Distribution',\n",
    "        'yscale': 'log',\n",
    "    }\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist_tweets():\n",
    "    configs = [\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['retweet_count'],\n",
    "            'title': 'Retweet Counts',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['reply_count'],\n",
    "            'title': 'Replay Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['favorite_count'],\n",
    "            'title': 'Favorite Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_hashtags'],\n",
    "            'title': 'Hashtag Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_urls'],\n",
    "            'title': 'Url Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['num_mentions'],\n",
    "            'title': 'Mentions Counts',\n",
    "            'yscale': 'log',\n",
    "        },\n",
    "        {\n",
    "            'type': 'hist',\n",
    "            'column': tweets['created_at'][tweets['created_at'] > pd.Timestamp(OUTLIER_TIMESTAMP)],\n",
    "            'title': 'Tweets Creation Date Distribution',\n",
    "            'yscale': 'log',\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    build_grid_plot(configs=configs)\n",
    "\n",
    "plot_hist_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all numerical columns there are outliers to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def boxplot_tweets_show(yscale='linear'):\n",
    "    configs = [\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['retweet_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['reply_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['favorite_count'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_hashtags'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_urls'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "        {\n",
    "            'type': 'boxplot',\n",
    "            'df': tweets,\n",
    "            'columns': ['num_mentions'],\n",
    "            'yscale': yscale,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    build_grid_plot(configs=configs)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def replace_outliers(df, column_name, threshold):\n",
    "    column = df[column_name]\n",
    "    to_replace = len(column[column > threshold])\n",
    "    perc_to_replace = to_replace / len(column) * 100\n",
    "    lg.info(f'{to_replace} ({perc_to_replace}%) element replaced for column {column_name}')\n",
    "    median = column.median()\n",
    "    df[column_name] = column.parallel_map(lambda x: median if x > threshold else x)\n",
    "\n",
    "replace_outliers(tweets, 'retweet_count', 6e5)\n",
    "replace_outliers(tweets, 'reply_count', 6e4)\n",
    "replace_outliers(tweets, 'favorite_count', 1.2e5)\n",
    "replace_outliers(tweets, 'num_hashtags', 1e4)\n",
    "replace_outliers(tweets, 'num_urls', 1e4)\n",
    "replace_outliers(tweets, 'num_mentions', 1e5)\n",
    "\n",
    "boxplot_tweets_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_num_cols = ['statuses_count', 'bot']\n",
    "users.astype({'bot': 'int64'}).corr(method='pearson', numeric_only=True).loc[users_num_cols, users_num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_num_cols = ['user_id', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']\n",
    "tweets.corr(method='pearson', numeric_only=True).loc[tweets_num_cols, tweets_num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "tweets.plot.scatter(x='reply_count', y='favorite_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cat_tweets = users.drop(columns=tweets_num_cols)\n",
    "\n",
    "X_pca = PCA(n_components=2).fit_transform(cat_tweets)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "plt.title(\"PCA\")\n",
    "plt.xlabel(\"1st eigenvector\")\n",
    "plt.ylabel(\"2nd eigenvector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> TODO prof recap points for DATA UNDERSTANDING\n",
    "(last slide of data understanding)\n",
    "Checklist for Data Understanding\n",
    "- Determine the quality of the data.(e.g.syntactic accuracy)\n",
    "- Find outliers. (e. g. using visualization techniques)\n",
    "- Detect and examine missing values. Possible hidden by default values.\n",
    "- Discover new or confirm expected dependencies or correlations between attributes.\n",
    "- Check specific application dependent assumptions (e.g. the attribute follows a normal distribution)\n",
    "- Compare statistics with the expected behaviour.\n",
    "-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
