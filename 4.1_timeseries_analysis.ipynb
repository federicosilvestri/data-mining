{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DM - Time series analysis [TASK 4.1]\n",
    "\n",
    "Extrapolation of success score timeseries and analysis about them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Library imports and initial settings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import zlib\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax, TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.piecewise import SymbolicAggregateApproximation\n",
    "from tslearn.shapelets import ShapeletModel, grabocka_params_to_shapelet_size_dict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import scipy.stats as stats\n",
    "\n",
    "from utils import fetch_preprocessed_dataset, store_preprocessed_dataset, build_grid_plot\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging as lg\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "root = lg.getLogger()\n",
    "root.setLevel(lg.INFO)\n",
    "\n",
    "handler = lg.StreamHandler(sys.stdout)\n",
    "handler.setLevel(lg.DEBUG)\n",
    "formatter = lg.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\n",
    "nb_workers = int(os.cpu_count() / 2 + 1)\n",
    "\n",
    "pandarallel.initialize(\n",
    "    progress_bar=True,\n",
    "    nb_workers=nb_workers,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetching users and tweets dataset saved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the users and tweets dataset from pickle\n",
    "dataset = fetch_preprocessed_dataset(step_name=\"outlier_detection\")\n",
    "users = dataset['users.pickle']\n",
    "tweets = dataset['tweets.pickle']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11508 entries, 0 to 11507\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              11508 non-null  int64         \n",
      " 1   name            11508 non-null  string        \n",
      " 2   lang            11508 non-null  string        \n",
      " 3   bot             11508 non-null  bool          \n",
      " 4   created_at      11508 non-null  datetime64[ns]\n",
      " 5   statuses_count  11508 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), string(2)\n",
      "memory usage: 550.7 KB\n"
     ]
    }
   ],
   "source": [
    "users.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10266779 entries, 0 to 13664695\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   user_id         object        \n",
      " 1   retweet_count   float64       \n",
      " 2   reply_count     float64       \n",
      " 3   favorite_count  float64       \n",
      " 4   num_hashtags    float64       \n",
      " 5   num_urls        float64       \n",
      " 6   num_mentions    float64       \n",
      " 7   created_at      datetime64[ns]\n",
      " 8   text            string        \n",
      "dtypes: datetime64[ns](1), float64(6), object(1), string(1)\n",
      "memory usage: 783.3+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove tweets with invalid `user_id`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "user_ids = set([str(i) for i in users['id'].values.tolist()])\n",
    "\n",
    "tweets = tweets[tweets['user_id'].map(lambda i: i in user_ids)].astype({'user_id': 'int64'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tweets filtering by date and sorting by it. Here are used tweets of year 2019."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2010486), Label(value='0 / 2010486…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d593b825d09d42a69f60b550d2bd4577"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "             user_id  retweet_count  reply_count  favorite_count  \\\n8384098   2240858066            0.0          0.0             0.0   \n6210422   2240858066            0.0          0.0             0.0   \n6852918   2240858066            4.0          0.0             0.0   \n192212     494302461            1.0          0.0             0.0   \n7159403    494302461            8.0          0.0             7.0   \n...              ...            ...          ...             ...   \n7460655   2275740517            1.0          0.0             1.0   \n12234385  2273349732            0.0          0.0             0.0   \n9099401   2275248397            0.0          0.0             0.0   \n9440580   2274254095            0.0          0.0             0.0   \n7783606   2287318956            0.0          0.0             3.0   \n\n          num_hashtags  num_urls  num_mentions          created_at  \\\n8384098            0.0       0.0           0.0 2019-01-01 00:21:18   \n6210422            0.0       0.0           0.0 2019-01-01 00:23:50   \n6852918            0.0       0.0           1.0 2019-01-01 00:24:30   \n192212             0.0       0.0           0.0 2019-01-01 00:47:32   \n7159403            0.0       0.0           0.0 2019-01-01 00:48:03   \n...                ...       ...           ...                 ...   \n7460655            0.0       0.0           0.0 2019-11-15 22:02:30   \n12234385           0.0       0.0           0.0 2019-11-15 22:03:55   \n9099401            0.0       0.0           0.0 2019-11-15 22:05:08   \n9440580            0.0       0.0           0.0 2019-11-15 22:10:43   \n7783606            0.0       0.0           0.0 2019-11-15 22:11:22   \n\n                                                       text  \n8384098               tava me sentindo super mal esses dias  \n6210422   Ãs vezes tudo o que a gente precisa pra se se...  \n6852918   RT @myh3ro: TO FAZENDO TODO MUNDO ASSISTIR SHE...  \n192212    Ã cosÃ¬, mente e cuore sono anarchici nella l...  \n7159403   Le cose piÃ¹ belle non sono perfette, sono spe...  \n...                                                     ...  \n7460655   Anche la mia mamma quando ero piccolo mi sgrid...  \n12234385  LE DIFFICOLTA' SONO COME LA CARTAIGIENICA... N...  \n9099401   Tra 20 anni sarai deluso per ciÃ² che non hai ...  \n9440580   Vuoi sapere chi sei? Non chiedertelo, agisci: ...  \n7783606   Prendi una donna e fai l'uomo. Rispettala semp...  \n\n[4474637 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>retweet_count</th>\n      <th>reply_count</th>\n      <th>favorite_count</th>\n      <th>num_hashtags</th>\n      <th>num_urls</th>\n      <th>num_mentions</th>\n      <th>created_at</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8384098</th>\n      <td>2240858066</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-01-01 00:21:18</td>\n      <td>tava me sentindo super mal esses dias</td>\n    </tr>\n    <tr>\n      <th>6210422</th>\n      <td>2240858066</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-01-01 00:23:50</td>\n      <td>Ãs vezes tudo o que a gente precisa pra se se...</td>\n    </tr>\n    <tr>\n      <th>6852918</th>\n      <td>2240858066</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2019-01-01 00:24:30</td>\n      <td>RT @myh3ro: TO FAZENDO TODO MUNDO ASSISTIR SHE...</td>\n    </tr>\n    <tr>\n      <th>192212</th>\n      <td>494302461</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-01-01 00:47:32</td>\n      <td>Ã cosÃ¬, mente e cuore sono anarchici nella l...</td>\n    </tr>\n    <tr>\n      <th>7159403</th>\n      <td>494302461</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-01-01 00:48:03</td>\n      <td>Le cose piÃ¹ belle non sono perfette, sono spe...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7460655</th>\n      <td>2275740517</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-11-15 22:02:30</td>\n      <td>Anche la mia mamma quando ero piccolo mi sgrid...</td>\n    </tr>\n    <tr>\n      <th>12234385</th>\n      <td>2273349732</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-11-15 22:03:55</td>\n      <td>LE DIFFICOLTA' SONO COME LA CARTAIGIENICA... N...</td>\n    </tr>\n    <tr>\n      <th>9099401</th>\n      <td>2275248397</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-11-15 22:05:08</td>\n      <td>Tra 20 anni sarai deluso per ciÃ² che non hai ...</td>\n    </tr>\n    <tr>\n      <th>9440580</th>\n      <td>2274254095</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-11-15 22:10:43</td>\n      <td>Vuoi sapere chi sei? Non chiedertelo, agisci: ...</td>\n    </tr>\n    <tr>\n      <th>7783606</th>\n      <td>2287318956</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-11-15 22:11:22</td>\n      <td>Prendi una donna e fai l'uomo. Rispettala semp...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4474637 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INIT_2019_TIMESTAMP = pd.Timestamp('2019-01-01')\n",
    "INIT_2020_TIMESTAMP = pd.Timestamp('2020-01-01')\n",
    "\n",
    "mask_tweets_2019 = tweets['created_at'].parallel_map(lambda t: INIT_2019_TIMESTAMP <= t < INIT_2020_TIMESTAMP)\n",
    "tweets = tweets[mask_tweets_2019]\n",
    "tweets = tweets.sort_values(by=['created_at'])\n",
    "\n",
    "tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computation of indexes `acceptance_score` and `diffusion_score` to combine them in the `success_score`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "count    4.474637e+06\nmean     1.337116e+02\nstd      4.083013e+03\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      9.090909e-01\nmax      1.823150e+06\nName: success_score, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acceptance_score = tweets['retweet_count'] + tweets['reply_count'] + tweets['favorite_count']\n",
    "diffusion_score = tweets['num_hashtags'] + tweets['num_mentions'] + tweets['num_urls']\n",
    "\n",
    "tweets[\"success_score\"] = acceptance_score / (diffusion_score + 0.1)\n",
    "\n",
    "tweets[\"success_score\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useless columns removal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "users = users[[\"id\", \"bot\"]]\n",
    "tweets = tweets[[\"user_id\", \"success_score\", \"created_at\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Success score timeseries column addition. Here it's created a new column for each users with the list of `success_score` ordered by `created_at` date. Each of these list has length of days in a year (365)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1358), Label(value='0 / 1358'))), …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f8aa5c5b2fc43ae9aa33b680b4fb26e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def time_series_by_days(group):\n",
    "    result = [-1 for _ in range(365)]\n",
    "    success_score_serie = group.groupby(tweets[\"created_at\"].dt.date)[\"success_score\"].sum()\n",
    "    for index, success_score in success_score_serie.items():\n",
    "        result[pd.Period(index, freq='D').day_of_year - 1] = success_score\n",
    "    return result\n",
    "\n",
    "tweets_grouped_by_users = tweets.groupby([\"user_id\"]).parallel_apply(time_series_by_days)\n",
    "\n",
    "users = users.merge(tweets_grouped_by_users.to_frame(\"success_score\"), left_on='id', right_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`users` preprocessed dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Users dataset base statistics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lg.info(f'number of time series: {users.shape[0]}')\n",
    "lg.info(f'size of each time serie: {len(users[\"success_score\"][0])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of some time series colored according related class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, user in list(users.iterrows())[:80]:\n",
    "    success_score = user[\"success_score\"]\n",
    "    color = \"r\" if user[\"bot\"] else \"c\"\n",
    "    plt.plot(range(1, len(success_score) + 1), success_score, c=color)\n",
    "\n",
    "plt.legend(['Bot', 'User'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save time series users pickle\n",
    "store_preprocessed_dataset(step_name=\"timeseries\", file_name=\"users.pickle\", df=users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the users dataset from pickle\n",
    "dataset = fetch_preprocessed_dataset(step_name=\"timeseries\")\n",
    "users = dataset['users.pickle']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function able to extract numpy matrix from `success_score` column of `users` dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_success_score(dt):\n",
    "    return np.array([t for t in dt[\"success_score\"].values])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time series clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`X` is the matrix representing the clustering dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = extract_success_score(users)\n",
    "\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is defined the Compression-based Dissimilarity Measure (CDM) metric to evaluate models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cdm_metric(x, y):\n",
    "    x_str = (' '.join([str(v) for v in x.ravel()])).encode('utf-8')\n",
    "    y_str = (' '.join([str(v) for v in y.ravel()])).encode('utf-8')\n",
    "    return len(zlib.compress(x_str + y_str)) / (len(zlib.compress(x_str)) + len(zlib.compress(y_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abstraction of grid search to run it with different clustering models and plot results. This algorithm try to run models with different params and plots results to provide the programmer a way to decide the best parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gridsearch_plot(create_model, params, x, labels=None, metric_callback=None):\n",
    "    xvalues = []\n",
    "    inertias = []\n",
    "    metric_callback = metric_callback if metric_callback is not None else (lambda m: m.inertia_)\n",
    "    for i, param in enumerate(tqdm_notebook(params)):\n",
    "        model = create_model(param)\n",
    "        model.fit(x)\n",
    "        metric = metric_callback(model)\n",
    "        if metric is not None:\n",
    "            xvalues.append(param[\"xvalue\"] if \"xvalue\" in param else i)\n",
    "            inertias.append(metric)\n",
    "    plt.plot(xvalues, inertias)\n",
    "    if labels is not None:\n",
    "        plt.xlabel(labels[0])\n",
    "        plt.ylabel(labels[1])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clustering dataset scaling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KMeans for time series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time series kmeans grid search to find the best k."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_kmeans_ts = [{'k': i, 'xvalue': i} for i in list(range(2, 31))]\n",
    "def create_kmeans_ts(param):\n",
    "    return TimeSeriesKMeans(n_clusters=param[\"k\"], metric=\"euclidean\", max_iter=100, n_init=10, n_jobs=-1)\n",
    "\n",
    "gridsearch_plot(create_kmeans_ts, params_kmeans_ts, X_scaled, (\"K\", \"Euclidean\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model construction and fitting with best k found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans_ts = create_kmeans_ts({\"k\": 13})\n",
    "kmeans_ts.fit(X_scaled)\n",
    "\n",
    "lg.info(f'Euclidean: {kmeans_ts.inertia_}')\n",
    "lg.info(f'CDM: {cdm_metric(X_scaled, kmeans_ts.labels_)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cluster centers discovered plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_centers = kmeans_ts.cluster_centers_\n",
    "plt.plot(cluster_centers.reshape(cluster_centers.shape[1], -1))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of function to plot distribution among clusters and usage of it for time series kmeans model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_clusters_distribution(model):\n",
    "    labels_distribution = np.unique(model.labels_, return_counts=True)\n",
    "    plt.bar(labels_distribution[0], labels_distribution[1])\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters_distribution(kmeans_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of function to plot distribution among clusters and bot label and usage of it for time series kmeans model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_bot_distribution(model):\n",
    "    users_pct = pd.crosstab(model.labels_, users['bot'].map(lambda x: 'Bot' if x else 'User'))\n",
    "\n",
    "    users_pct.plot(kind='bar', stacked=False, title='bot x cluster')\n",
    "    plt.xlabel('cluster')\n",
    "    plt.ylabel('bot')\n",
    "    plt.show()\n",
    "    print(users_pct)\n",
    "\n",
    "plot_bot_distribution(kmeans_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Symbolic Aggregate Approximation (SAX)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sax compressed time series creation from clustering dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_segments = 20\n",
    "sax = SymbolicAggregateApproximation(n_segments=n_segments, alphabet_size_avg=8)\n",
    "X_sax = sax.fit_transform(X)\n",
    "\n",
    "X_sax.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of some comparisons of timeseries and their sax compression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_num_plots = 4\n",
    "\n",
    "rows = max_num_plots // 2 + 1\n",
    "plt.figure(figsize=(20, rows * 5))\n",
    "offset = X.shape[1] / (n_segments - 1)\n",
    "for i in range(1, max_num_plots + 1):\n",
    "    plt.subplot(rows, 2, i)\n",
    "    plt.plot(X[i], label=\"Raw\")\n",
    "    plt.plot([i * offset for i in range(X_sax[i].shape[0])], X_sax[i].squeeze(), label=\"Sax\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sax dataset scaling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_sax_scaled = scaler.fit_transform(X_sax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sax time series grid search."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gridsearch_plot(create_kmeans_ts, params_kmeans_ts, X_sax_scaled, (\"K\", \"Euclidean\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creation of kmeans model with the best k and fitting of it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans_sax_ts = create_kmeans_ts({\"k\": 8})\n",
    "kmeans_sax_ts.fit(X_sax_scaled)\n",
    "\n",
    "lg.info(f'Euclidean: {kmeans_sax_ts.inertia_}')\n",
    "lg.info(f'CDM: {cdm_metric(X_sax_scaled, kmeans_sax_ts.labels_)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of cluster centers found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_centers = kmeans_sax_ts.cluster_centers_\n",
    "plt.plot(cluster_centers.reshape(cluster_centers.shape[1], -1))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of distribution of sax time series with respect clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_clusters_distribution(kmeans_sax_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of distribution of sax time series according bot labels and clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bot_distribution(kmeans_sax_ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature-based clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for features calculation by time series."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_features(data):\n",
    "    return pd.DataFrame.from_records([{\n",
    "        \"mean\": np.mean(values),\n",
    "        \"std\": np.std(values),\n",
    "        \"variance\": np.var(values),\n",
    "        \"median\": np.median(values),\n",
    "        \"p10\": np.percentile(values, 10),\n",
    "        \"p25\": np.percentile(values, 25),\n",
    "        \"p50\": np.percentile(values, 50),\n",
    "        \"p75\": np.percentile(values, 75),\n",
    "        \"p90\": np.percentile(values, 90),\n",
    "        \"iqr\": np.percentile(values, 75) - np.percentile(values, 25),\n",
    "        \"covariance\": 1.0 * np.mean(values) / np.std(values),\n",
    "        \"skew\": stats.skew(values),\n",
    "        \"kurtosis\": stats.kurtosis(values)\n",
    "    } for values in data]).astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Features calculation with respect `X` and scaling of it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = MinMaxScaler().fit_transform(calculate_features(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KMeans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid search of kmeans model on time series features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_kmeans = [{'k': i, 'xvalue': i} for i in list(range(2, 31))]\n",
    "def create_kmeans(param):\n",
    "    return KMeans(n_clusters=param[\"k\"], max_iter=100, n_init=10)\n",
    "\n",
    "gridsearch_plot(create_kmeans, params_kmeans, features, (\"K\", \"SSE\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kmeans model creation with best k found and fitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans = create_kmeans({\"k\": 7})\n",
    "kmeans.fit(features)\n",
    "\n",
    "lg.info(f'SSE: {kmeans.inertia_}')\n",
    "lg.info(f'Silhouette score: {silhouette_score(features, kmeans.labels_)}')\n",
    "lg.info(f'CDM: {cdm_metric(features, kmeans.labels_)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features colored by related cluster and centers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scatter_plt_pairs = [\n",
    "    (0, 2),\n",
    "    (2, 10),\n",
    "    (1, 11),\n",
    "    (1, 12)\n",
    "]\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'type': 'scatter',\n",
    "        'df': pd.DataFrame(features),\n",
    "        'labels': kmeans.labels_,\n",
    "        'centers': kmeans.cluster_centers_,\n",
    "        'x_index': x_index,\n",
    "        'y_index': y_index,\n",
    "    } for x_index, y_index in scatter_plt_pairs\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features distribution among clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_clusters_distribution(kmeans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features distribution according clusters and bot label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bot_distribution(kmeans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DBSCAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DBSCAN grid search on features dataset to find best `min_samples`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_dbscan = [{'min_samples': i, 'xvalue': i} for i in [10, 20, 50, 100, 150, 200, 500, 1000]]\n",
    "def create_dbscan(param):\n",
    "    return DBSCAN(eps=0.1, min_samples=param[\"min_samples\"])\n",
    "\n",
    "def metric_callback(model):\n",
    "    # check if there are at least 2 different values\n",
    "    if len(set(model.labels_)) >= 2:\n",
    "        # negative metric\n",
    "        return - silhouette_score(features, model.labels_)\n",
    "\n",
    "gridsearch_plot(create_dbscan, params_dbscan, features, (\"Min samples\", \"Silhouette score\"), metric_callback=metric_callback)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of knee method used to find the best eps according best k found previously."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_knee_method(df, k):\n",
    "    dist = squareform(pdist(df, 'euclidean'))\n",
    "    kth_distances = [d[np.argsort(d)[k]] for d in dist]\n",
    "    plt.plot(range(0, len(kth_distances)), sorted(kth_distances))\n",
    "    plt.ylabel(f'dist from {k}th neighbor')\n",
    "    plt.xlabel('sorted distances')\n",
    "    plt.grid(True)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "k = 20\n",
    "plot_knee_method(features, k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DBSCAN model creation with best params and fitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.02, min_samples=k)\n",
    "dbscan.fit(features)\n",
    "\n",
    "lg.info(f'Silhouette score: {silhouette_score(features, dbscan.labels_)}')\n",
    "lg.info(f'CDM: {cdm_metric(features, dbscan.labels_)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features colored by clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "        'type': 'scatter',\n",
    "        'df': pd.DataFrame(features),\n",
    "        'labels': dbscan.labels_,\n",
    "        'x_index': x_index,\n",
    "        'y_index': y_index,\n",
    "    } for x_index, y_index in scatter_plt_pairs\n",
    "]\n",
    "\n",
    "build_grid_plot(configs=configs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features distribution among clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_clusters_distribution(dbscan)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of features distribution according clusters and bot label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bot_distribution(dbscan)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shapelets discovery"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset splitting to create train and test set and related set and labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = users.copy()\n",
    "label = dataset.pop(\"bot\")\n",
    "\n",
    "train_set, test_set, train_label, test_label = train_test_split(dataset, label, stratify=label, test_size=0.20)\n",
    "train_set, test_set = [extract_success_score(t) for t in [train_set, test_set]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling of input sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMeanVariance()\n",
    "scaler.fit(train_set, test_set)\n",
    "\n",
    "train_set_scaled = scaler.transform(train_set)\n",
    "test_set_scaled = scaler.transform(test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Heuristic function used to find best shapelets quantity and dimensions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_classes = len(users[\"bot\"].value_counts())\n",
    "\n",
    "shapelet_sizes = grabocka_params_to_shapelet_size_dict(\n",
    "    n_ts=train_set_scaled.shape[0],\n",
    "    ts_sz=train_set_scaled.shape[1],\n",
    "    n_classes=n_classes,\n",
    "    l=0.01,                                     # fraction of time series to use in base shapelet\n",
    "    r=5                                         # number of different shapelets\n",
    ")\n",
    "\n",
    "shapelet_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to create shapelet model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_shapelet_model(params={}, max_iter=100, verbose=True):\n",
    "    return ShapeletModel(\n",
    "        n_shapelets_per_size=shapelet_sizes,\n",
    "        max_iter=max_iter,\n",
    "        batch_size=512,\n",
    "        verbose=verbose,\n",
    "        **params,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid search on shapelet model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    optimizer=[\"sgd\", \"adam\"],\n",
    "    weight_regularizer=[0.001, 0.0001],\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(estimator=create_shapelet_model(verbose=False), param_grid=params, cv=4, n_jobs=-1, verbose=True)\n",
    "grid_search.fit(train_set_scaled, train_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print of best params and best scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shapelet model creation using best parameters selected and fitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shapelet_model = create_shapelet_model(grid_search.best_params_, max_iter=300)\n",
    "\n",
    "shapelet_model.fit(train_set_scaled, train_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shapelet model prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_labels = shapelet_model.predict(test_set_scaled)\n",
    "lg.info(f\"Accuracy: {accuracy_score(test_label, predicted_labels)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shapelets plots and analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of all found shapelets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for shapelet in shapelet_model.shapelets_:\n",
    "    plt.plot(shapelet)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot of some time series and related shapelets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_locations = shapelet_model.locate(test_set)\n",
    "n_shapelets = sum(shapelet_sizes.values())\n",
    "\n",
    "for ts_id in range(4):\n",
    "    plt.plot(test_set[ts_id])\n",
    "    for i, shapelet in enumerate(shapelet_model.shapelets_):\n",
    "        t0 = predicted_locations[ts_id, i]\n",
    "        xvalues = [(t0 + j) for j in range(len(shapelet))]\n",
    "        plt.plot(xvalues, shapelet)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
